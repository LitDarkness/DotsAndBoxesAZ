class Config:
    # -----------------------------------------
    # 1. 棋盘与游戏基础设置
    # -----------------------------------------
    # 建议先从 3x3 开始验证，训练速度快。
    # 只要模型学会了 3x3 的 "Double Cross" 策略，
    # 改成 4x4 或 5x5 只需要改这里并重新训练即可。
    BOARD_ROWS = 3
    BOARD_COLS = 3
    
    # 动作空间计算 (自动计算，勿改)
    # 横边: (Rows+1)*Cols, 竖边: Rows*(Cols+1)
    NUM_HORIZONTAL_EDGES = (BOARD_ROWS + 1) * BOARD_COLS
    NUM_VERTICAL_EDGES = BOARD_ROWS * (BOARD_COLS + 1)
    ACTION_SIZE = NUM_HORIZONTAL_EDGES + NUM_VERTICAL_EDGES

    # -----------------------------------------
    # 2. 自我博弈 (Self-Play) 设置 - CPU 负载区
    # 你的 Ultra 9 285HX 性能很强，可以适当提高质量
    # -----------------------------------------
    # 总迭代次数：AlphaZero 是一个长期过程，建议设大一点，随时可以 Ctrl+C 停止
    NUM_SELF_PLAY_ITERS = 200  
    
    # 每次迭代产生的对局数
    # 有了滑动窗口后，每轮不需要生成太多，50-100 局即可，保证迭代速度快，反馈频繁
    NUM_EPISODES = 50  
    
    # MCTS 模拟次数 (每走一步棋思考多少次)
    # 训练时：100-200 足够，目的是快速生成数据
    # 比赛/评估时：建议 400+
    NUM_MCTS_SIMS = 200
    
    # 探索系数 (通常 1.0 - 1.5)
    C_PUCT = 1.1

    # -----------------------------------------
    # 3. 神经网络训练设置 - GPU 负载区
    # -----------------------------------------
    # 学习率
    LEARNING_RATE = 0.001
    
    # 每次训练的 Epochs
    # 因为有滑动窗口，数据会重复利用，Epochs 不宜过大，防止过拟合
    EPOCHS = 10
    
    # 批次大小
    # 设为 256 甚至 512 都能瞬间跑完，能极大加速 GPU 吞吐
    BATCH_SIZE = 256  
    
    # -----------------------------------------
    # 4. 训练策略优化 (匹配 trainer.py)
    # -----------------------------------------
    # 滑动窗口大小 (History Window)
    # 保留最近多少次迭代的数据。
    # 20 iters * 50 episodes = 1000 局棋作为训练池
    TRAIN_WINDOW_SIZE = 20 
    
    # 竞技场评估场次
    # 新老模型对战次数，必须是偶数 (双方各执先手一半)
    ARENA_COMPARE_GAMES = 26
    
    # 胜率更新阈值 (0.55 表示新模型胜率超过 55% 才替换)
    UPDATE_THRESHOLD = 0.4999